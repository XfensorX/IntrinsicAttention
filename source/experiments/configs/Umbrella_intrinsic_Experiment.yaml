defaults:
  - override hydra/launcher: ray

hydra:
  sweeper:
    params:
      seed: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10
      env.length: 2, 5, 10, 20, 50

env:
  name: Umbrella
  length: 100

# Common Parameters
seed: 42  # Default seed (overridden by sweeper)
env_steps: 100000

# PPO Learner Config
ppo_learner:
  minibatch_size: 1000
  lr: 0.0060984166342
  num_epochs: 1
  shuffle_batch_per_epoch: true
  num_learners: 0

# Intrinsic Reward Module Config
intrinsic_learner:
  minibatch_size: 1000
  shuffle_batch_per_epoch: true
  num_epochs: 1
  lr: 0.0002715581955
  intrinsic_reward_coeff: 0.2795015916508  # Weight of intrinsic rewards

# Training settings
training:
  gamma: 1
  train_batch_size_per_learner: 5000
  grad_clip: 0.5

# PPO Algorithm Parameters
ppo:
  use_critic: true
  use_gae: true
  lambda_: 0.9485408017724
  use_kl_loss: true
  kl_coeff: 0.161583927863
  kl_target: 0.0201019014258
  vf_loss_coeff: 0.7790510010087
  entropy_coeff: 0.0015431216992
  clip_param: 0.2459212356676
  vf_clip_param: 0.3769744131561
  extrinsic_value_hidden_size_1: 127
  extrinsic_value_hidden_size_2: 60
  policy_head_hidden_size: 32
  value_head_hidden_size: 101
  embedding_hidden_size: 121
  embedding_dim: 62

# Environment Runner Settings
env_runners:
  num_env_runners: 0
  num_cpus_per_env_runner: 1
  num_envs_per_env_runner: 1
  gym_env_vectorize_mode: "SYNC"
  rollout_fragment_length: "auto"
  batch_mode: "complete_episodes"

# Evaluation Settings
evaluation:
  evaluation_interval: 1
  evaluation_duration: 20
  evaluation_duration_unit: "episodes"
  evaluation_force_reset_envs_before_iteration: true
  evaluation_num_env_runners: 1

# System resources
num_cpus_for_main_process: 2
data_dir: ./experiment_data/UmbrellaIntrinsicAttentionPPO


