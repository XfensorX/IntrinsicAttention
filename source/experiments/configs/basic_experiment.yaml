defaults:
  - override hydra/launcher: ray

#TODO: Check reproducibility. Setting seed via debugging and just on env with one learner might work
hydra:
  sweeper:
    params:
      seed: 1, 2, 3


# HP explanation: https://docs.ray.io/en/latest/rllib/package_ref/algorithm-config.html#rllib-config-evaluation
environment:
  name: "CartPole-v1"

training:
  lr: 3.0e-4
  gamma: 0.99
  num_epochs: 3
  minibatch_size: 1000
  shuffle_batch_per_epoch: true
  train_batch_size_per_learner: 2000
  # PPO-specific
  use_critic: true
  use_gae: true
  lambda_: 0.95
  use_kl_loss: true
  kl_coeff: 0.2
  kl_target: 0.01
  vf_loss_coeff: 1.0
  entropy_coeff: 0.0
  clip_param: 0.2
  vf_clip_param: 0.5
  grad_clip: 0.5

learners:
  # 0 = local learner on driver; set >0 to use remote learners
  num_learners: 0
  num_cpus_per_learner: "auto"

env_runners:
  num_env_runners: 0
  num_envs_per_env_runner: 1
  gym_env_vectorize_mode: "SYNC"
  num_cpus_per_env_runner: 1
  rollout_fragment_length: "auto"
  batch_mode: "complete_episodes"

evaluation:
  # Im Results.json sind f√ºr jede Iteration Ergebnisse, sie werden dann aber nur intervall weise geupdated
  evaluation_interval: 1
  evaluation_duration: 50
  evaluation_duration_unit: "episodes"
  evaluation_force_reset_envs_before_iteration: True
  evaluation_num_env_runners: 0

num_cpus_for_main_process: 1
seed: 0
env_steps: 10000
data_dir: "./data/"

