@INPROCEEDINGS{selfattentionbasedtemporalintrinsicreward,
  author={Jiang, Zhuo and Tian, Daiying and Yang, Qingkai and Peng, Zhihong},
  booktitle={2021 China Automation Congress (CAC)},
  title={Self-Attention based Temporal Intrinsic Reward for Reinforcement Learning},
  year={2021},
  volume={},
  number={},
  pages={2022-2026},
  doi={10.1109/CAC53003.2021.9727314}
}


@inproceedings{liang2021rllib,
    title={{RLlib} Flow: Distributed Reinforcement Learning is a Dataflow Problem},
    author={
        Wu, Zhanghao and
        Liang, Eric and
        Luo, Michael and
        Mika, Sven and
        Gonzalez, Joseph E. and
        Stoica, Ion
    },
    booktitle={Conference on Neural Information Processing Systems ({NeurIPS})},
    year={2021},
    url={https://proceedings.neurips.cc/paper/2021/file/2bce32ed409f5ebcee2a7b417ad9beed-Paper.pdf}
}

@inproceedings{rliable,
  author       = {Rishabh Agarwal and
                  Max Schwarzer and
                  Pablo Samuel Castro and
                  Aaron C. Courville and
                  Marc G. Bellemare},
  editor       = {Marc'Aurelio Ranzato and
                  Alina Beygelzimer and
                  Yann N. Dauphin and
                  Percy Liang and
                  Jennifer Wortman Vaughan},
  title        = {Deep Reinforcement Learning at the Edge of the Statistical Precipice},
  booktitle    = {Advances in Neural Information Processing Systems 34: Annual Conference
                  on Neural Information Processing Systems 2021, NeurIPS 2021, December
                  6-14, 2021, virtual},
  pages        = {29304--29320},
  year         = {2021},
  url          = {https://proceedings.neurips.cc/paper/2021/hash/f514cec81cb148559cf475e7426eed5e-Abstract.html},
  biburl       = {https://dblp.org/rec/conf/nips/AgarwalSCCB21.bib},
}

@article{hydra,
  title={Hydra: A framework for elegantly configuring complex applications},
  author={Ross, Omri D. I. and L'Hermitte, J{\'e}rome},
  journal={Journal of Open Source Software},
  volume={5},
  number={52},
  pages={2415},
  year={2020},
  doi={10.21105/joss.02415},
  publisher={The Open Journal}
}

@inproceedings{umbrella,
  author       = {Ian Osband and
                  Yotam Doron and
                  Matteo Hessel and
                  John Aslanides and
                  Eren Sezener and
                  Andre Saraiva and
                  Katrina McKinney and
                  Tor Lattimore and
                  Csaba Szepesv{\'{a}}ri and
                  Satinder Singh and
                  Benjamin Van Roy and
                  Richard S. Sutton and
                  David Silver and
                  Hado van Hasselt},
  title        = {Behaviour Suite for Reinforcement Learning},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=rygf-kSYwH},
  timestamp    = {Mon, 15 May 2023 16:24:40 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/OsbandDHASSMLSS20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{decoupling,
  author       = {Tianwei Ni and
                  Michel Ma and
                  Benjamin Eysenbach and
                  Pierre{-}Luc Bacon},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/9dc5accb1e4f4a9798eae145f2e4869b-Abstract-Conference.html},
  timestamp    = {Fri, 01 Mar 2024 16:26:20 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/NiMEB23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{pignatelli2024surveytemporalcreditassignment,
  author       = {Eduardo Pignatelli and
                  Johan Ferret and
                  Matthieu Geist and
                  Thomas Mesnard and
                  Hado van Hasselt and
                  Laura Toni},
  title        = {A Survey of Temporal Credit Assignment in Deep Reinforcement Learning},
  journal      = {Trans. Mach. Learn. Res.},
  volume       = {2024},
  year         = {2024},
  url          = {https://openreview.net/forum?id=bNtr6SLgZf},
  timestamp    = {Thu, 08 Aug 2024 15:22:39 +0200},
  biburl       = {https://dblp.org/rec/journals/tmlr/PignatelliFGMHT24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lirpg,
author = {Zheng, Zeyu and Oh, Junhyuk and Singh, Satinder},
title = {On learning intrinsic rewards for policy gradient methods},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In many sequential decision making tasks, it is challenging to design reward functions that help an RL agent efficiently learn behavior that is considered good by the agent designer. A number of different formulations of the reward-design problem have been proposed in the literature. In this paper we build on the Optimal Rewards Framework of Singh et al. [2010] that defines the optimal intrinsic reward function as one that when used by an RL agent achieves behavior that optimizes the task-specifying or extrinsic reward function. Previous work in this framework has shown how good intrinsic reward functions can be learned for lookahead search based planning agents. Whether it is possible to learn intrinsic reward functions for learning agents remains an open problem. In this paper we derive a novel algorithm for learning intrinsic rewards for policy-gradient based learning agents. We compare the performance of an augmented agent that uses our algorithm to provide additive intrinsic rewards to an A2C-based policy learner (for Atari games) and a PPO-based policy learner (for Mujoco domains) with a baseline agent that uses the same policy learners but with only extrinsic rewards. We also compare our method with using a constant "live bonus" and with using a count-based exploration bonus (i.e., pixel-SimHash). Our results show improved performance on most but not all of the domains.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {4649–4659},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{smac,
  author  = {Marius Lindauer and Katharina Eggensperger and Matthias Feurer and André Biedenkapp and Difan Deng and Carolin Benjamins and Tim Ruhkopf and René Sass and Frank Hutter},
  title   = {SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {54},
  pages   = {1--9},
  url     = {http://jmlr.org/papers/v23/21-0888.html}
}